{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# Traduit et décomposé par Camille Besse\n",
    "# License: BSD 3 clause\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared, DotProduct, ConstantKernel\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "#pylab.rcParams.keys()\n",
    "fg = (24,8)\n",
    "cm_points = ListedColormap(['#FF0000','#FFFFFF', '#00FF00','#000000', '#0000FF'])\n",
    "cm = 'jet_r'\n",
    "params = {'figure.titlesize': 'xx-large',\n",
    "          'font.size': '12',\n",
    "          'text.color': 'k',\n",
    "          'figure.figsize': fg,\n",
    "         }\n",
    "pylab.rcParams.update(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Comparaison des Algorithmes de Classification\n",
    "---\n",
    "\n",
    "Une comparaison va être proposée entre plusieurs classificateurs de la librairie Python SciKit-Learn sur des ensembels de données synthétiques. Le but de cet exercice est d'illustrer la nature des frontières de décision des différents algorithems de classification. Faites attention cependant, les intuitions qui se dégagent de ces exemples pourraient ne pas se transferer sur des ensembles de données rééls.\n",
    "\n",
    "En particulier avec des données de grande dimensionnalité, celles-ci peuvent être facilement séparées linérairement, et des algorithmes simples comme le Naive Bayes ou les SVMs linéaires peuvent donner de meilleurs résultats en terme de généralisation que ce que donneraient des algorthmes plus complexes ou avec plus de capacité. \n",
    "\n",
    "Les fonctions pour permettre l'affichage vont présenter les données d'entrainement comme des points solides annotés de leur couleur. Les points de tests seront semi-transparents avec leur couleur \"vraie\". La valeur en bas à droite donne la précision de l'algorithme sur l'ensemble de données de test.\n",
    "\n",
    "source traduite : http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "<!-- Source : \n",
    "A comparison of a several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets. \n",
    "\n",
    "Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classifiers.\n",
    "\n",
    "The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set. -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ensembles de données\n",
    "---\n",
    "\n",
    "Les trois ensembles de données que nous allons utiliser sont les suivants : \n",
    "- Les lunes\n",
    "- les cercles \n",
    "- un ensemble linéairement séparable\n",
    "\n",
    "Nous pouvons mettre différent niveaux de bruits dans ces données. Voyons ce que ca donne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres initiaux des données : \n",
    "## Pour le générateur de nombre pseu-aléatoires et la reproductibilité des résultats\n",
    "rng_seed = 0 \n",
    "## Le bruit dans les données (entre 0 et 0.5)\n",
    "bruit = 0.\n",
    "# ---\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=rng_seed, n_clusters_per_class=2, flip_y=bruit/10,class_sep=2-2*bruit)\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "X += bruit * 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "names = [\"Lunes\", \"Cercles\", \"Séparables\"]\n",
    "datasets = [make_moons(noise=bruit, random_state=rng_seed),\n",
    "            make_circles(noise=bruit/2, factor=0.5, random_state=rng_seed),\n",
    "            linearly_separable\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons maintenant ces datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des graphiques\n",
    "figure = plt.figure(figsize=fg)\n",
    "i = 1\n",
    "\n",
    "# Pour chaque dataset (compteur, DataSet)\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # Préparation rapide des données : normalisation des données et calcul des bornes \n",
    "    X, y = ds\n",
    "    h = 0.2\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Visualisons le data set \n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(1, len(datasets), i)\n",
    "    \n",
    "    ax.set_title(names[ds_cnt])\n",
    "        \n",
    "    # Affichons les ensembles de données\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_points, edgecolors='k')\n",
    "\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    ## EXERCICE : Jouez avec les paramètres de bruit initiaux pour voir les différences dans les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définissons un ensemble de fonctions qui vont nous aider à visualiser plus ismplement les résultats des différents classificateurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creationMesh(X):\n",
    "    \"\"\"\n",
    "    Crée un grille sur un espace bidimensionnel. Prends le min et le max de chaque dimension et calcule la grille avec une résolution de 0.02. \n",
    "    X: un vecteur à deux colonnes de données. \n",
    "    \"\"\"\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx,yy\n",
    "\n",
    "\n",
    "def plotClassifierOnData(name,clf,data,i=3,n=1,multi=False):\n",
    "    \"\"\"\n",
    "    Pour Afficher les récultat d'un classificateur sur un dataset\n",
    "    name : le titre du graphique\n",
    "    clf : le classificateur à utiliser\n",
    "    data : les données à utiliser\n",
    "    i : Le ième graphique sur n à afficher (pour afficher 3 graphiques par ligne)\n",
    "    n : Le nombre total de graphiques à afficher\n",
    "    multi: détermine si on affiche juste la frontière de décision (true) ou \n",
    "           le score/proba de chaque point de l'espace car on ne peut afficher le score en multiclasse.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Préparation rapide des données : \n",
    "    # normalisation des données \n",
    "    X, y = data\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    # Séparation des données en TRAIN - TEST\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=rng_seed)\n",
    "    # Pour la visualisation des régions et calcul des bornes \n",
    "    xx,yy = creationMesh(X)\n",
    "\n",
    "    # creation du bon nombre de figures à afficher (3 par lignes)\n",
    "    ax = plt.subplot(n/3,3,i)\n",
    "    \n",
    "    # entrainement du classificateur et calcul du score final (accuracy)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "\n",
    "    # Pour afficher les frontières de décision on va choisir une color pour \n",
    "    # chacun des points x,y du mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "    # Si on est en multiclasse (2 ou +) on affiche juste les frontières\n",
    "    if multi:\n",
    "         Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:# sinon on peut afficher le gradient du score\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "    # On affiche le mesh de décision\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    test = ax.contourf(xx, yy, Z, 100, cmap=cm, alpha=.8)\n",
    "\n",
    "    #On affiche la légende\n",
    "    cbar = plt.colorbar(test)\n",
    "    cbar.ax.set_title('score')\n",
    "    \n",
    "    # On affiche les points d'entrainement\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_points,\n",
    "               edgecolors='k',s=100)\n",
    "    # Et les points de test\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_points, \n",
    "               edgecolors='k',marker='X',s=100)\n",
    "\n",
    "    # on définit les limites des axes et autres gogosses\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "#     ax.set_xticks(())\n",
    "#     ax.set_yticks(())\n",
    "    ax.set_title(name,fontsize=22)\n",
    "    # dont le score en bas à droite\n",
    "    ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "            size=15, horizontalalignment='right')\n",
    "\n",
    "\n",
    "def plotClassifier(name, clf, datasets):\n",
    "    \"\"\"\n",
    "    Affiche pour un classificateur donné, son résultat sur l'ensemble des datasets préalablement déterminés\n",
    "    name : le nom du classificateur à afficher (titre du graphique)\n",
    "    clf : un classificateur de scikit-learn\n",
    "    datasets : une liste de datasets\n",
    "    \"\"\"\n",
    "    f = plt.figure(figsize=fg)\n",
    "    # Pour chacun des DataSet\n",
    "    for ds_cnt, ds in enumerate(datasets):\n",
    "        plotClassifierOnData(name, clf, ds,ds_cnt+1,3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Algorithmes de Classification\n",
    "---\n",
    "\n",
    "Voyons maintenant les différents algorithmes et leur(s) paramètre(s):\n",
    "1. <a href=\"#knn\">KNN</a>\n",
    "2. <a href=\"#cart\">Arbre de décision</a>\n",
    "3. <a href=\"#rf\">Random Forest</a>\n",
    "4. <a href=\"#ada\">AdaBoost</a>\n",
    "5. <a href=\"#boost\">Gradient Boosting</a>\n",
    "6. <a href=\"#rl\">Régression logistique</a>\n",
    "7. <a href=\"#nn\">Réseaux de neurones</a>\n",
    "8. <a href=\"#svm\">SVM(s)</a>\n",
    "9. <a href=\"#gp\">Processus Gaussiens</a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. K-Nearest Neighbors (KNN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"knn\"/>\n",
    "\n",
    "\n",
    "Ou les K plus proches voisins. \n",
    "\n",
    "[`sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’, metric_params=None, n_jobs=None)`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "Les principaux paramètres de la classe `sklearn.neighbors.KNeighborsClassifier` sont:\n",
    "- `n_neighbors` : le nombre de voisins considérés\n",
    "- `weights` : `uniform` (tous les poids sont égaux), `distance` (le poids est inversement proportionnel à la distance par rapport à l'échantillon testé) ou toute autre fonction lambda définie par l'utilisateur;\n",
    "- `algorithm` : `brute`, `ball_tree`, `KD_tree` ou `auto`. Dans le premier cas, les voisins les plus proches pour chaque cas de test sont calculés par une recherche sur la grille sur l'ensemble d'apprentissage. Dans les deuxième et troisième cas, les distances entre les exemples sont stockées dans un arbre pour accélérer la recherche des voisins les plus proches. Si vous définissez ce paramètre sur auto, la bonne façon de trouver les voisins sera automatiquement choisie en fonction du jeu d’entraînement.\n",
    "- `leaf_size` : seuil pour passer à la recherche sur la grille si l'algorithme de recherche de voisins est `ball_tree` ou `KD_tree`;\n",
    "- `metric` : fonction de calcul de la distance netre les points : `minkowski` (par défaut), `manhattan` (si `p`= 1), `euclidean` (si `p`= 2), `chebyshev` (si `p`= Infini).\n",
    "\n",
    "**Note :** souffre de la malédiction de la dimensionnalité. Pour des datasets de plusieurs millions de données dans de grandes dimensions, cf. [Annoy](https://github.com/spotify/annoy) la librairie de Spotify sur les K-NN approximés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex1 - Mise en oeuvre du kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: préparez vos hyperparamètres - Remplacez None\n",
    "\n",
    "plotClassifier(\"K Nearest Neighbors\", KNeighborsClassifier(None), datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Arbres de Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"cart\"/>\n",
    "\n",
    "\n",
    "[`sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "\n",
    "Les principaux paramètres de la classe sklearn.tree.DecisionTreeClassifier sont les suivants:\n",
    "- `max_depth` : la profondeur maximale de l'arbre;\n",
    "- `max_features` : nombre maximal de caractéristiques permettant de rechercher la meilleure partition (nécessaire avec un grand nombre de caractéristiques, car il serait \"coûteux\" de rechercher des partitions pour toutes les caractéristiques);\n",
    "- `min_samples_leaf` : nombre minimal d'échantillons dans une feuille. Ce paramètre empêche la création d’arbres où une feuille n’aurait que quelques membres.\n",
    "\n",
    "Les paramètres de l’arbre doivent être définis en fonction des données d’entrée. Cette opération est généralement effectuée au moyen d’une validation croisée (vue ce matin).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2 - Mise en oeuvre des arbres de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: préparez vos hyperparamètres - Remplacez None\n",
    "\n",
    "dec = DecisionTreeClassifier(None)\n",
    "plotClassifier(\"Arbres de Decision\",  dec, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "dec.fit(*datasets[1]) # datasets : 0 : Moons | 1 : Cercles | 2 : Lineairement Séparable\n",
    "\n",
    "export_graphviz(dec, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Forêt Aléatoire (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"rf\"/>\n",
    "\n",
    "\n",
    "Une forêt aléatoire est un ensemble d'arbres de décision qui peuvent éventuellement sur-apprendre chacun différemment (Bagging). \n",
    "\n",
    "[`sklearn.ensemble.RandomForestClassifier(n_estimators=’warn’, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "Voici les paramètres auxquels nous devons faire attention lorsque nous construisons une forêt :\n",
    "- `n_estimators` : le nombre d'arbres dans la forêt;\n",
    "- `max_features` : le nombre de caractéristiques à considérer lors de la recherche du meilleur partage;\n",
    "- `min_samples_leaf` : nombre minimal d'échantillons requis pour être au niveau d'un nœud feuille;\n",
    "- `max_depth` : la profondeur maximale de l'arbre;\n",
    "- `criterion` : la fonction utilisée pour mesurer la qualité d'une scission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex3 - Mise en oeuvre des forêts aléatoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: préparez vos hyperparamètres - Remplacez None\n",
    "\n",
    "plotClassifier(\"Forêt Aléatoire\",  RandomForestClassifier(None), datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"ada\"/>\n",
    "\n",
    "\n",
    "Adaboost utilise également un ensemble d'arbres de décision mais qui sous-apprennent (ils ont une profondeur de 1 par défaut).\n",
    "\n",
    "[`sklearn.ensemble.AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm=’SAMME.R’, random_state=None)`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
    "\n",
    "Voici les paramètres auxquels nous devons faire attention lorsque nous construisons un modèle Adaboost:\n",
    "- `n_estimators` : le nombre d'arbres utilisés;\n",
    "- `learning_rate` : le taux d'apprentissage, i.e. la variation sur le poids des exemples mal classés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex4 - Mise en oeuvre d'Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: préparez vos hyperparamètres - Remplacez None\n",
    "\n",
    "plotClassifier(\"AdaBoost\", AdaBoostClassifier(None), datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"boost\"/>\n",
    "\n",
    "\n",
    "Gradient Boosting est une version généralisée d'Adaboost. \n",
    "\n",
    "[`sklearn.ensemble.GradientBoostingClassifier(loss=’deviance’, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=’friedman_mse’, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort=’auto’, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "\n",
    "Les paramètres auxquels nous devons faire attention sont les mêmes que lorsque nous construisons un modèle Adaboost :\n",
    "- `n_estimators` : le nombre d'arbres utilisés;\n",
    "- `learning_rate` : le taux d'apprentissage, i.e. la variation sur le poids des exemples mal classés;\n",
    "\n",
    "Auxquels ont peut ajouter \n",
    "- `subsample` : La fraction d'examples à utiliser pour ajuster les classificateurs (`bootstraping`). Si inférieur à 1.0, l'algorithme devient du [`Stochastic Gradient Boosting`](https://en.wikipedia.org/wiki/Gradient_boosting#Stochastic_gradient_boosting).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex5 - Mise en oeuvre du gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: préparez vos hyperparamètres - Remplacez None\n",
    "\n",
    "clf = GradientBoostingClassifier(None)\n",
    "\n",
    "plotClassifier(\"Boosting de gradient\",  clf, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Regression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"rl\"/>\n",
    "\n",
    "\n",
    "Ou comment faire de la régression linéaire avec des classes.\n",
    "\n",
    "[`sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None)`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "Les principaux paramètres de la régression logistique sont les suivants:\n",
    "- `penalty` : La norme ($L_1$ ou $L_2$) utilisée pour calculer la fonction de perte;\n",
    "- `max_iter` : nombre maximal d'itération ;\n",
    "- `C` : Le paramètre inverse de régularisation dans $[0,1]$, plus `C` est proche de 0, plus la régularisation est forte.\n",
    "\n",
    "On constate que dans le cas de ces datasets, la capacité simple du classificateur n'est pas suffisante, il faudrait faire une projection dans l'espace des caractéristiques pour augmenter sa capacité et ainsi améliorer l'exactitude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex6 - Mise en oeuvre de la regression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: préparez vos hyperparamètres - Remplacez None\n",
    "\n",
    "plotClassifier(\"Regression Logistique\",LogisticRegression(None), datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Perceptron Multi-Couches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"nn\"/>\n",
    "\n",
    "\n",
    "[`sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), activation=’relu’, solver=’adam’, alpha=0.0001, batch_size=’auto’, learning_rate=’constant’, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)`](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "\n",
    "Les principaux paramètres du perceptron sont les suivants:\n",
    "- `hidden_layer_sizes` : Le nombre de neurones sur la (ou les) couche(s) cachée(s).\n",
    "- `activation` : La fonction d'activation de chaque neurone.\n",
    "- `alpha` : Taux de régularisation $L_2$ sur les poids.\n",
    "- `max_iter`,`tol` : `max_iter` est le nombre d'itération avant d'arrêter si on ne souhaite pas attendre la convergence définie par la tolérance `tol` ( = 0.0001 par défaut).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex7 - Mise en oeuvre du MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: préparez vos hyperparamètres - Remplacez None\n",
    "\n",
    "clf = MLPClassifier(None)\n",
    "\n",
    "plotClassifier(\"Perceptron Multi-Couches\",  clf, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"svm\"/>\n",
    "\n",
    "\n",
    "Ou machines à vecteur de support (avec noyau linéaire ou gaussien).\n",
    "\n",
    "[`sklearn.svm.SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto_deprecated’, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "\n",
    "Les principaux paramètres du SVM sont les suivants:\n",
    "- `kernel` : le noyau à utiliser pour la mesure de distance;\n",
    "- `C` : régularisation des points de support;\n",
    "- `max_iter`,`tol` : `max_iter` est le nombre d'itération avant d'arrêter si on ne souhaite pas attendre la convergence définie par la tolérance `tol` ( = 0.001 par défaut)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex8 - Mise en oeuvre des SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: préparez vos hyperparamètres - Remplacez None\n",
    "\n",
    "plotClassifier(\"SVM avec noyau \"+noyau, SVC(None), datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Processus Gaussien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"gp\"/>\n",
    "\n",
    "\n",
    "[`sklearn.gaussian_process.GaussianProcessClassifier(kernel=None, optimizer=’fmin_l_bfgs_b’, n_restarts_optimizer=0, max_iter_predict=100, warm_start=False, copy_X_train=True, random_state=None, multi_class=’one_vs_rest’, n_jobs=None)`](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html)\n",
    "\n",
    "Le principal paramètre du Processus Gaussien est le `kernel` i.e. le noyau à utiliser pour la mesure de distance. On pourra essayer les différents noyaux définis dans le code.\n",
    "Il ne sera pas nécessaire d'optimiser les hyperparmaètres du noyau puisque l'apprentissage s'en occupe.\n",
    "\n",
    "Voici la documentation des différents noyaux : \n",
    "- [Constant](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ConstantKernel.html#sklearn.gaussian_process.kernels.ConstantKernel)\n",
    "- [Dot product](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.DotProduct.html#sklearn.gaussian_process.kernels.DotProduct)\n",
    "- [RBF](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF)\n",
    "- [Matern](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html#sklearn.gaussian_process.kernels.Matern) : RBF généralisé\n",
    "- [Rational Quadratic](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RationalQuadratic.html#sklearn.gaussian_process.kernels.RationalQuadratic) : mixture infinie de RBF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex9 - Mise en oeuvre des processus gaussiens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: préparez vos kernels - Remplacez None\n",
    "\n",
    "plotClassifier(\"Processus Gaussien\",  GaussianProcessClassifier(None), datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Algorithmes de Classification Multiclasses\n",
    "---\n",
    "\n",
    "On va changer de données : regardons les données de plusieurs types d'Iris (les fleurs):\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*2uGt_aWJoBjqF2qTzRc2JQ.jpeg\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "g = sns.pairplot(iris, hue='species', markers=['+','d','x'])\n",
    "plt.show()\n",
    "iris.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rappel : Séparer les données en deux : entrainement et test\n",
    "#### Avantages\n",
    "\n",
    "   - En séparant les données aléatoirement on ne teste pas sur les mêmes données que celles entrainées\n",
    "   - Cela garantit que nous n'utiliseront pas les mêmes observations dans les deux ensembles \n",
    "   - Cela généralise mieux les données plus rapidement\n",
    "\n",
    "#### Disadvantages\n",
    "   - Le score d'exactitude sur l'ensemble de test varie en fonction des données qui y ont été sélectionnées\n",
    "   - On peut mitiger ca en utilisant la validation croisée\n",
    "\n",
    "#### Notes\n",
    "   - Le score d'exactitude  des modèles dépend des observations de l'ensemble de test, qui est déterminé par l'initilisation  du générateur pseudo-aléatoire.\n",
    "   - Plus la complexité du modèle augmente, plus l'exactitude d'entrainement augmente\n",
    "   - Si un modèle est trop complexe (overfitting) ou pas assez complexe (underfitting), l'exactitude d'entrainement est trop basse. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex10 - Séparation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.drop('species', axis=1)\n",
    "y = iris['species']\n",
    "\n",
    "# TODO: Utilisez la fonction de scikit learn pour séparer le jeu de données en train/test\n",
    "\n",
    "X_train, X_test, y_train, y_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois séparés on peut valider quelle serait la meilleure valeur de $k$ dans un $k$-NN sur l'ensemble des données d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimentons avec différentes valeurs de k\n",
    "k_range = list(range(1,26))\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    scores.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    \n",
    "plt.plot(k_range, scores)\n",
    "plt.xlabel('Valeur de k')\n",
    "plt.ylabel('Exactitude')\n",
    "plt.title('Exactitude pour différentes valeurs de k dans les k-NN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Choisir les caractéristiques en entrée: \n",
    "c0 = 'sepal_length'\n",
    "c1 = 'sepal_width'\n",
    "c2 = 'petal_length'\n",
    "c3 = 'petal_width'\n",
    "\n",
    "f1 = c0\n",
    "f2 = c1\n",
    "#  ----------------------\n",
    "\n",
    "# Paramétrer les classificateurs\n",
    "clfs = [\n",
    "    KNeighborsClassifier(n_neighbors=3, weights='uniform', leaf_size=30),\n",
    "    LogisticRegression(random_state=rng_seed,max_iter=1000,penalty='l2',C=0.01),\n",
    "    SVC(kernel='rbf', C=1,gamma=.2,max_iter=1000),\n",
    "    DecisionTreeClassifier(max_depth=6, random_state=rng_seed),\n",
    "    RandomForestClassifier(max_depth=6, n_estimators=15, max_features=2, random_state=rng_seed),\n",
    "    AdaBoostClassifier(n_estimators=15, learning_rate=0.5,random_state=rng_seed),\n",
    "    MLPClassifier(hidden_layer_sizes=(5,), activation='relu', alpha=0.01, max_iter=1000, random_state=rng_seed),\n",
    "    GaussianProcessClassifier(kernel=noyauRBF),\n",
    "    GradientBoostingClassifier(n_estimators=15, learning_rate=0.5, subsample=1, max_depth=2, random_state=rng_seed),\n",
    "]\n",
    "# Liste des noms associés\n",
    "clf_names = [\n",
    "    'k-NN','Régression Logistique','SVM',\n",
    "    'Arbre de décision','Forêt aléatoire','AdaBoost',\n",
    "    'Perceptron','Processus Gaussien','Gradient Boosting'   \n",
    "]\n",
    "\n",
    "#  ----------------------\n",
    "\n",
    "# Création du dataset \n",
    "X = iris[[f1,f2]].values\n",
    "\n",
    "## Encodage des espèces en valeurs numériques pour la coloration\n",
    "le = LabelEncoder()\n",
    "le.fit(iris.species.unique())\n",
    "y = le.transform(iris.species)\n",
    "data_iris = [X,y]\n",
    "#  ----------------------\n",
    "\n",
    "# Affichage des frontières de décision pour nos neuf classificateurs \n",
    "f = plt.figure(figsize= (20,20))\n",
    "\n",
    "# Pour chacun des Classificateurs\n",
    "for cnt, clf in enumerate(clfs):\n",
    "    plotClassifierOnData(clf_names[cnt], OneVsRestClassifier(clf), data_iris,cnt+1,len(clfs),True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons les matrices de confusion pour ces classificateurs : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(clf,classes,                       \n",
    "                          title,i,n,X_train,y_train,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Cette fonction calcule et affiche la matrice de confusion pour un classificateur donné.\n",
    "    clf : classificateur\n",
    "    classes : les classes à déterminer dans le dataset\n",
    "    title : \n",
    "    \"\"\"\n",
    "    y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "    # Calcul de la matrice de confusion\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    cnf_matrix  = cnf_matrix .astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    ax = plt.subplot(n/3,3,i)\n",
    "    test = ax.imshow(cnf_matrix, interpolation='nearest', cmap=cmap)\n",
    "    ax.set_title(title)\n",
    "    cbar = plt.colorbar(test)\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_xticklabels(classes)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_yticklabels(classes)\n",
    "\n",
    "    thresh = cnf_matrix.max() / 2.\n",
    "    for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "        plt.text(j, i, format(cnf_matrix[i, j], '.2f'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cnf_matrix[i, j] > thresh else \"black\",size=12)\n",
    "\n",
    "    ax.set_ylabel('Vrai label')\n",
    "    ax.set_xlabel('label Prédit')\n",
    "\n",
    "# Affichage des matrices de confusion pour nos neuf classificateurs \n",
    "f = plt.figure(figsize= (20,20))\n",
    "\n",
    "# Pour chacun des Classificateurs\n",
    "for cnt, clf in enumerate(clfs):\n",
    "    plot_confusion_matrix(OneVsRestClassifier(clf), iris.species.unique(), clf_names[cnt],cnt+1,len(clfs),X_train,y_train)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## EX 11\n",
    "___\n",
    "\n",
    "Dans cet exercice vous avez les moyens de maintenant choisir le bon classificateur et ses hyperparamètres en utilisant toutes les connaissances de la journée.\n",
    "\n",
    "Nous allons charger un dataset contenant 4 types de fruits (Pommes, Oranges, Mandarines et Citrons) et c'est à vous de déterminer à partir de leur masse, hauteur, largeur, couleur, quels sont les types de fruits.\n",
    "\n",
    "Un exemple de code vous est donné pour pouvoir visualiser les données, mais rappelez vous qu'il est plus intéressant d'utiliser l'ensemble des données plutot que juste deux colonnes de celles-ci !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On élimine la colonne \"subtype\" qui pourra faire l'objet d'exercieces supplémntaires\n",
    "fruits = pd.read_table('fruits.txt').drop('fruit_subtype',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetons un oeuil aux données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(fruits.drop('fruit_label',axis=1), hue='fruit_name')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
